import spacy
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
from textblob import TextBlob
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
import torch

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Load models
nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer('all-MiniLM-L6-v2')
toxicity_classifier = pipeline("text-classification", model="unitary/toxic-bert", device_map="auto")

ner_tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-large")
ner_model = AutoModelForTokenClassification.from_pretrained("microsoft/deberta-v3-large", device_map="auto")

# --- Place your context_keywords, keyword_to_category, and category_descriptions dicts here ---

# Initial dictionaries
context_keywords = {
    "religion": [
        "god", "muslim", "islam", "hindu", "sikh", "buddhist", "christian", "jew", "judaism",
        "church", "mosque", "temple", "synagogue", "quran", "bible", "torah",
        "faith", "belief", "prayer", "worship", "pilgrimage", "ramadan", "easter", "diwali"
    ],
    "politics": [
        "election", "vote", "ballot", "referendum", "campaign", "president", "prime minister",
        "government", "administration", "policy", "law-making", "parliament", "congress",
        "senate", "cabinet", "politician", "diplomacy", "geopolitics", "public office"
    ],
    "health": [
        "vaccine", "vaccination", "immunization", "virus", "flu", "influenza", "covid", "pandemic",
        "cancer", "diabetes", "disease", "illness", "fever", "doctor", "physician", "nurse",
        "medicine", "drug", "medication", "hospital", "clinic", "public health",
        # controlled-substance examples
        "opioid", "opium", "heroin", "methamphetamine", "meth", "hash", "cocaine"
    ],
    "gender": [
        "woman", "women", "man", "men", "female", "male", "girl", "boy",
        "gender", "sex", "feminist", "feminism", "patriarchy",
        "misogyny", "misandry", "lgbt", "lgbtq", "queer", "nonbinary",
        "trans", "transgender", "cisgender", "pronoun"
    ],
    "race": [
        "race", "racism", "racist", "ethnicity", "minority", "majority",
        "black", "white", "asian", "african", "caucasian", "latino", "hispanic",
        "indigenous", "native", "brown", "mixed-race", "xenophobia"
    ],
    "law": [
        "legal", "illegal", "lawful", "unlawful", "court", "lawsuit", "litigation",
        "judge", "jury", "verdict", "sentence", "police", "arrest", "detention",
        "crime", "criminal", "felony", "misdemeanor", "regulation", "legislation", "statute", "attorney", "lawyer"
    ],
    "money": [
        "money", "bank", "banking", "finance", "financial", "economy", "economic",
        "bitcoin", "crypto", "cryptocurrency", "blockchain",
        "dollar", "euro", "pound", "rupee", "yen", "yuan", "peso",
        "poverty", "wealth", "income", "salary", "wage", "tax", "taxation",
        "investment", "stock", "market"
    ],
    "truth": [
        "truth", "true", "false", "lie", "lying", "fact", "factual",
        "proof", "evidence", "verify", "verification", "authentic", "debunk",
        "hoax", "myth", "misinformation", "disinformation", "rumor"
    ],
    "privacy": [
        "privacy", "private", "data", "userdata", "security", "cybersecurity",
        "breach", "hack", "leak", "surveillance", "tracking", "monitoring",
        "personal information", "gdpr", "encryption", "anonymity"
    ],
    "fairness": [
        "fair", "fairness", "equity", "equality", "justice",
        "bias", "unbiased", "discrimination", "inclusive", "inclusion", "accessibility"
    ],
    "lgbtq": [
        "gay", "lesbian", "bisexual", "transgender", "trans", "queer",
        "lgbt", "lgbtq", "pride", "homosexual", "coming out", "rainbow flag"
    ],
    "environment": [
        "environment", "environmental", "climate", "climate change", "global warming",
        "carbon", "carbon footprint", "co2", "emissions", "pollution", "air quality",
        "sustainability", "recycle", "recycling", "renewable", "greenhouse", "deforestation",
        "biodiversity", "ecosystem", "plastic waste"
    ],
    # NEW CATEGORY
    "cities": [
        # Americas
        "new york", "los angeles", "san francisco", "chicago", "toronto", "vancouver",
        "mexico city", "são paulo", "rio de janeiro", "buenos aires",
        # Europe
        "london", "paris", "berlin", "madrid", "rome", "amsterdam",
        "vienna", "zurich", "athens", "copenhagen", "stockholm", "oslo", "helsinki",
        "moscow", "istanbul",
        # Middle East & Africa
        "dubai", "abu dhabi", "doha", "riyadh", "jeddah", "tehran",
        "cairo", "lagos", "nairobi", "johannesburg",
        # Asia-Pacific
        "beijing", "shanghai", "hong kong", "shenzhen", "tokyo", "osaka",
        "seoul", "singapore", "bangkok", "kuala lumpur", "jakarta",
        "mumbai", "delhi", "bangalore", "chennai", "kolkata",
        "sydney", "melbourne", "auckland"
    ],
    "general": []
}

keyword_to_category = {
    "religion": [
        "religious_intolerance", "hate_speech", "cultural_sensitivity",
        "stereotyping", "toxicity", "harassment",
        "blasphemy", "radicalization", "religious_propaganda",
        "conversion_pressure"
    ],
    "politics": [
        "political_extremism", "partisanship", "misinformation",
        "disinformation", "bias", "toxicity", "propaganda",
        "election_interference", "polarization", "foreign_influence",
        "public_policy_debate"
    ],
    "gender": [
        "gender_bias", "stereotyping", "cultural_sensitivity",
        "hate_speech", "discrimination", "toxicity", "harassment",
        "misgendering", "deadnaming", "gender_based_violence",
        "gender_erasure"
    ],
    "race": [
        "racial_bias", "hate_speech", "discrimination",
        "stereotyping", "toxicity", "harassment",
        "racial_slur", "white_supremacy", "racial_profiling",
        "segregation_advocacy"
    ],
    "lgbtq": [
        "sexual_orientation_bias", "hate_speech", "cultural_sensitivity",
        "discrimination", "toxicity", "harassment",
        "conversion_therapy_promotion", "anti_pride",
        "transphobia", "misgendering", "deadnaming"
    ],
    "nationality": [
        "national_origin_bias", "xenophobia", "discrimination",
        "hate_speech", "harassment",
        "national_stereotyping", "immigration_exclusion",
        "border_security_extremism", "cultural_eradication"
    ],
    "age": [
        "ageism", "age_bias", "discrimination", "stereotyping", "harassment",
        "elder_abuse", "child_mistreatment", "age_based_exclusion"
    ],
    "disability": [
        "ableism", "disability_bias", "discrimination",
        "cultural_sensitivity", "stereotyping", "harassment",
        "eugenics", "inaccessibility", "mockery_of_disability",
        "medical_misrepresentation"
    ],
    "health": [
        "misinformation", "disinformation", "harmful_advice",
        "factual_inaccuracy", "safety", "self_harm_content",
        "medical_advice", "unverified_treatments",
        "anti_vax_content", "pseudo_science"
    ],
    "truth": [
        "truthfulness", "factuality", "hallucination",
        "misinformation", "epistemic_uncertainty", "defamation",
        "misleading_context", "cherry_picking", "out_of_context",
        "conspiracy_theory", "deepfake_deception"
    ],
    "factuality": [
        "truthfulness", "factual_inaccuracy", "hallucination",
        "misinformation", "defamation",
        "outdated_information", "citation_needed", "urban_legend"
    ],
    "environment": [
        "climate_misinformation", "environmental_ethics",
        "cultural_sensitivity", "propaganda",
        "climate_denial", "greenwashing", "eco_terrorism",
        "deforestation_promotion"
    ],
    "law": [
        "legality", "ethics", "privacy", "fairness",
        "compliance", "defamation",
        "legal_advice", "libel", "slander", "jury_tampering",
        "court_orders", "privacy_rights_violation"
    ],
    "crime": [
        "illegality", "criminal_advice", "harm", "misinformation",
        "cybercrime", "weapon_instructions", "violent_planning",
        "money_laundering_guide", "forgery"
    ],
    "violence": [
        "incitement", "harm", "violence_content", "toxicity",
        "safety", "graphic_violence", "terrorist_content",
        "violent_threat", "intimidation", "weapon_promotion",
        "self_harm_instructions"
    ],
    "drugs": [
        "harmful_advice", "misinformation", "illegality",
        "safety", "addiction_promotion", "drug_use_tutorial",
        "illegal_pharmacy", "overdose_content", "harm_reduction"
    ],
    "money": [
        "financial_fairness", "fraud", "scams", "ethics",
        "misinformation", "market_manipulation", "ponzi_scheme",
        "get_rich_quick", "gambling_promotion", "bribery",
        "money_laundering"
    ],
    "privacy": [
        "privacy_violation", "security", "data_leakage",
        "safety", "doxxing", "identity_theft", "credential_harvest",
        "location_tracking", "face_recognition",
        "biometric_data_leak"
    ],
    "technology": [
        "privacy", "security", "misinformation", "bias",
        "data_leakage", "deepfake_content", "tracking_software",
        "surveillance_capitalism", "malware_distribution"
    ],
    "ai": [
        "alignment", "fairness", "bias", "safety",
        "hallucination", "misuse", "model_exploitation",
        "jailbreak_instructions", "model_inversion",
        "data_poisoning", "unsafe_model_recommendation"
    ],
    "alignment": [
        "alignment", "value_misalignment", "ethics",
        "safe_behaviour", "reward_hacking", "unintended_behavior",
        "goal_misalignment", "specification_gaming",
        "wireheading", "side_effects"
    ],
    "toxicity": [
        "toxicity", "hate_speech", "harm", "offensiveness",
        "harassment", "slur_usage", "cyberbullying",
        "name_calling", "threatening_language"
    ],
    "sexuality": [
        "sexual_content", "hate_speech", "cultural_sensitivity",
        "discrimination", "sexual_exploitation", "minor_protection",
        "explicit_pornography", "non_consent", "incest",
        "bestiality", "fetish_content"
    ],
    "mental_health": [
        "harmful_advice", "misinformation", "stereotyping",
        "sensitivity", "self_harm_content", "triggering_content",
        "incorrect_diagnosis", "gaslighting", "stigmatizing_language"
    ],
    "fairness": [
        "fairness", "bias", "equality", "justice",
        "discrimination", "algorithmic_manipulation",
        "algorithmic_bias", "proxy_discrimination",
        "unfair_targeting", "redlining"
    ],
    "self_harm": [
        "self_harm_content", "suicidal_ideation", "harmful_advice",
        "self_harm_methods", "suicide_pact", "encouraging_self_harm"
    ],
    "spam": [
        "unsolicited_content", "scams", "fraud",
        "phishing", "spam_content", "link_farm", "clickbait"
    ]
}


# Category descriptions for intent detection
category_descriptions = {
    # ---- Identity, Culture, and Group Harms ----
    "religious_intolerance": "Content attacking, mocking, or inciting hostility toward specific religions or their followers.",
    "hate_speech": "Content that dehumanizes, vilifies, or incites violence against individuals or groups based on protected characteristics.",
    "cultural_sensitivity": "Content that requires careful treatment of cultural beliefs, practices, or taboos to avoid offense.",
    "stereotyping": "Content assigning generalized or reductive traits to an entire group, leading to bias or discrimination.",
    "toxicity": "Language or behavior that is abusive, inflammatory, or intended to provoke negative emotional responses.",
    "harassment": "Targeted and persistent abusive language, threats, or unwanted contact toward an individual or group.",
    "blasphemy": "Content showing disrespect, contempt, or mockery toward sacred figures or religious texts.",
    "radicalization": "Content that encourages or facilitates the adoption of extremist ideologies or justifies violence.",
    "religious_propaganda": "Efforts to convert, proselytize, or manipulate through religious messages.",
    "conversion_pressure": "Coercing or unduly pressuring individuals to change religious affiliation.",
    "gender_bias": "Unfair, prejudicial attitudes or actions directed at someone because of their gender identity or expression.",
    "discrimination": "Unjust or prejudicial treatment based on attributes like race, gender, sexuality, disability, or nationality.",
    "misgendering": "Referring to someone with words or pronouns that do not reflect their gender identity.",
    "deadnaming": "Using a transgender person’s previous name after they have changed it.",
    "gender_based_violence": "Threats, harassment, or violence targeting someone because of their gender or gender identity.",
    "gender_erasure": "Ignoring, invalidating, or denying the existence of specific gender identities.",
    "racial_bias": "Prejudice, stereotyping, or discrimination based on race or ethnicity.",
    "racial_slur": "Use of offensive terms or language targeting a racial or ethnic group.",
    "white_supremacy": "Ideologies, messages, or symbols asserting the dominance or superiority of white people.",
    "racial_profiling": "Suspicion or targeting of individuals based on racial or ethnic characteristics.",
    "segregation_advocacy": "Promotion of policies or attitudes supporting the separation of groups by race or ethnicity.",
    "sexual_orientation_bias": "Negative attitudes, discrimination, or violence against individuals based on sexual orientation.",
    "conversion_therapy_promotion": "Advocacy for practices intended to change an individual's sexual orientation or gender identity.",
    "anti_pride": "Content targeting or undermining LGBTQ+ pride, rights, or celebrations.",
    "transphobia": "Hostility, prejudice, or discriminatory acts toward transgender or gender non-conforming people.",
    "national_origin_bias": "Discriminatory or hostile attitudes based on nationality or perceived foreignness.",
    "xenophobia": "Fear, hatred, or intolerance of people perceived as outsiders or foreigners.",
    "national_stereotyping": "Assigning fixed traits or negative characteristics to people based on nationality.",
    "immigration_exclusion": "Advocacy for exclusion, expulsion, or restriction of immigrants.",
    "border_security_extremism": "Calls for or glorification of excessively harsh or violent border policies.",
    "cultural_eradication": "Calls to erase, assimilate, or destroy the culture or language of a group.",
    "ageism": "Prejudicial attitudes or discrimination based on age (youth or elder).",
    "age_bias": "Bias or stereotypes regarding people’s abilities or roles based on age.",
    "elder_abuse": "Physical, psychological, or financial abuse of elderly individuals.",
    "child_mistreatment": "Abuse, neglect, or exploitation of minors.",
    "age_based_exclusion": "Policies or practices that unfairly exclude individuals due to age.",
    "ableism": "Discrimination, prejudice, or stereotypes directed at people with disabilities.",
    "disability_bias": "Negative attitudes or actions based on someone’s disability.",
    "eugenics": "Advocacy for controlling reproduction based on desired traits; devaluing people with disabilities.",
    "inaccessibility": "Neglect or refusal to accommodate people with disabilities.",
    "mockery_of_disability": "Ridicule or humiliation of people with physical or mental disabilities.",
    "medical_misrepresentation": "False or misleading representation of disabilities or related medical information.",
    "minor_protection": "Safeguarding minors from abuse, exploitation, or inappropriate content.",
    "sexual_exploitation": "Abuse, coercion, or trafficking for sexual purposes.",

    # ---- Misinformation, Truth, and Safety ----
    "misinformation": "False or misleading information presented as fact, regardless of intent.",
    "disinformation": "Deliberately false information intended to deceive or manipulate.",
    "harmful_advice": "Suggestions or instructions that could lead to physical, emotional, or financial harm.",
    "factual_inaccuracy": "Statements that are demonstrably incorrect or not supported by credible evidence.",
    "self_harm_content": "Descriptions, encouragement, or instructions for self-harm, suicide, or dangerous acts.",
    "medical_advice": "Guidance on diagnosing, treating, or preventing health conditions, often without credentials.",
    "unverified_treatments": "Recommendations for medical treatments lacking scientific evidence.",
    "anti_vax_content": "Content discouraging vaccination or spreading myths about vaccines.",
    "pseudo_science": "Claims or theories lacking scientific support but presented as fact.",
    "truthfulness": "Commitment to accuracy and integrity in conveying information.",
    "factuality": "Degree to which statements align with verified facts and sources.",
    "hallucination": "Fabricated, incorrect, or fictional information generated by AI models.",
    "epistemic_uncertainty": "Expressions of uncertainty about the accuracy, source, or completeness of information.",
    "defamation": "False statements presented as fact that harm a person or organization’s reputation.",
    "misleading_context": "Presenting information in a way that distorts its intended meaning.",
    "cherry_picking": "Selecting only supporting evidence while ignoring contrary facts.",
    "out_of_context": "Quoting or referencing information in a way that misrepresents its original meaning.",
    "conspiracy_theory": "Claims of secret, large-scale plots without credible supporting evidence.",
    "deepfake_deception": "Manipulated audio, video, or images designed to impersonate, deceive, or defame.",
    "outdated_information": "Facts that may have been true but are no longer current or accurate.",
    "citation_needed": "Claims lacking verifiable references or reliable sources.",
    "urban_legend": "Widely circulated but untrue stories presented as fact.",
    "climate_misinformation": "False or misleading claims about climate change, its impacts, or causes.",
    "environmental_ethics": "Discussion of environmental responsibility and moral obligations toward the planet.",
    "climate_denial": "Denial or minimization of the scientific consensus on climate change.",
    "greenwashing": "Misleading claims that exaggerate the environmental benefits of products or actions.",
    "eco_terrorism": "Promotion or endorsement of violence in the name of environmental causes.",
    "deforestation_promotion": "Encouraging the large-scale removal of forests, often disregarding ecological impacts.",
    "factual_harm": "Spreading damaging or injurious falsehoods.",

    # ---- Law, Crime, Ethics ----
    "legality": "Content discussing whether specific actions comply with the law.",
    "ethics": "Considerations of right and wrong in personal or societal conduct.",
    "privacy": "Matters related to protection of personal information and autonomy.",
    "privacy_violation": "Unauthorized access, sharing, or publication of private or confidential information.",
    "data_leakage": "Unintentional or unauthorized exposure of sensitive data.",
    "safety": "Content raising concerns about health, well-being, or physical security.",
    "compliance": "Adherence to laws, regulations, or policies.",
    "legal_advice": "Specific recommendations or interpretation of laws and legal obligations.",
    "libel": "Written defamation or published falsehoods about an individual or group.",
    "slander": "Spoken defamation or falsehoods harming someone's reputation.",
    "jury_tampering": "Illegal attempts to influence the impartiality of a jury.",
    "court_orders": "References to official directives issued by courts.",
    "privacy_rights_violation": "Breach of legal rights to privacy and confidentiality.",
    "illegality": "Encouragement, description, or endorsement of unlawful acts.",
    "criminal_advice": "Guidance or encouragement for committing crimes.",
    "cybercrime": "Illegal activities using computers, networks, or digital systems.",
    "weapon_instructions": "Descriptions or instructions on making, acquiring, or using weapons.",
    "violent_planning": "Content outlining plans, strategies, or calls for violent acts.",
    "money_laundering_guide": "Instructions or strategies to disguise illegally obtained money.",
    "forgery": "Advice on creating fake documents, signatures, or credentials.",
    "incitement": "Encouragement or provocation to commit harmful, illegal, or violent acts.",
    "violence_content": "Descriptions, images, or videos depicting acts of physical violence.",
    "graphic_violence": "Gory, shocking, or explicit depictions of bodily harm.",
    "terrorist_content": "Promotion, glorification, or support of terrorism or terrorist organizations.",
    "violent_threat": "Explicit threats of physical harm toward individuals or groups.",
    "intimidation": "Use of threats, fear, or aggression to manipulate or control others.",
    "weapon_promotion": "Content advocating for or glorifying the use or possession of weapons.",
    "self_harm_instructions": "Guidance on methods of inflicting self-injury or suicide.",

    # ---- Money, Economy, Financial Harm ----
    "financial_fairness": "Discussion of equity, access, or justice within financial systems.",
    "fraud": "Deception intended for unlawful financial or personal gain.",
    "scams": "Schemes to defraud or deceive, typically for money.",
    "market_manipulation": "Deliberate actions to influence financial markets for personal benefit.",
    "ponzi_scheme": "Investment scams paying returns from new investors rather than legitimate profits.",
    "get_rich_quick": "Promotion of unrealistic or deceptive paths to quick wealth.",
    "gambling_promotion": "Encouragement or advertising of betting or games of chance.",
    "bribery": "Offering or accepting something of value in exchange for influence or action.",
    "money_laundering": "Concealing the origins of illegally obtained money.",
    "financial_advice": "Recommendations about investments, loans, or financial decisions, often without credentials.",

    # ---- Privacy, Security, and Technology ----
    "doxxing": "Publishing or threatening to publish private, identifying information without consent.",
    "identity_theft": "Acquiring and using another person's personal information for fraud.",
    "credential_harvest": "Collecting usernames, passwords, or other credentials without authorization.",
    "location_tracking": "Monitoring or disclosing someone’s location without consent.",
    "face_recognition": "Use of biometric technology to identify or surveil individuals.",
    "biometric_data_leak": "Exposure or mishandling of unique physical or behavioral characteristics.",
    "tracking_software": "Applications or code intended to monitor user behavior or collect data surreptitiously.",
    "surveillance_capitalism": "Commercial exploitation of personal data for profit, often without transparency.",
    "malware_distribution": "Creation or spread of software designed to disrupt, damage, or gain unauthorized access.",
    "security": "Measures and practices to protect systems, information, or individuals from threats.",

    # ---- AI, Algorithmic, and Technical Harms ----
    "alignment": "Degree to which AI outputs reflect intended goals, human values, or ethical boundaries.",
    "value_misalignment": "AI outputs that do not align with user intent or societal values.",
    "safe_behaviour": "Actions, recommendations, or outputs that reduce risk and harm.",
    "reward_hacking": "AI exploiting metrics, rules, or reward signals in unintended ways.",
    "unintended_behavior": "Unexpected or undesired actions or outputs from AI systems.",
    "goal_misalignment": "Mismatch between an AI system’s objectives and the user or societal goals.",
    "specification_gaming": "Manipulating system rules or rewards without achieving intended outcomes.",
    "wireheading": "AI self-stimulating its own reward signals rather than fulfilling its intended task.",
    "side_effects": "Unintended, potentially harmful consequences from an AI action.",
    "model_exploitation": "Attempts to circumvent safety features, jailbreak, or misuse AI models.",
    "jailbreak_instructions": "Guidance on bypassing safeguards or restrictions on systems.",
    "model_inversion": "Extracting private data from model outputs or queries.",
    "data_poisoning": "Tampering with training data to manipulate model behavior.",
    "unsafe_model_recommendation": "Advocacy or endorsement of AI models with known or likely risks.",
    "hallucination": "Fabricated, inaccurate, or fictional responses generated by AI.",
    "misuse": "Improper, dangerous, or malicious use of technology, platforms, or information.",
    "deepfake_content": "Synthetic media created to impersonate, defame, or mislead.",
    "algorithmic_manipulation": "Use of algorithms to unfairly influence, suppress, or amplify content.",
    "algorithmic_bias": "Systematic, unfair errors in algorithmic outputs due to biased data or design.",
    "proxy_discrimination": "Indirect discrimination using neutral attributes as stand-ins for protected characteristics.",
    "unfair_targeting": "Adverse treatment or exclusion based on group or personal characteristics.",
    "redlining": "Systemic denial of services based on group identity or geography.",

    # ---- Content Style, Harm, Exploitation ----
    "harm": "Language or actions likely to cause injury, distress, or negative consequences.",
    "offensiveness": "Statements or behavior likely to shock, insult, or upset others.",
    "slur_usage": "Use of offensive or derogatory terms targeting groups or individuals.",
    "cyberbullying": "Online aggression or harassment with intent to harm or intimidate.",
    "name_calling": "Insulting or mocking language directed at an individual.",
    "threatening_language": "Verbal or written threats of harm, violence, or retaliation.",
    "sexual_content": "Material describing or depicting explicit sexual acts or arousal.",
    "explicit_pornography": "Graphic and explicit depictions of sexual activity.",
    "non_consent": "Descriptions or depictions of sexual acts without clear agreement.",
    "incest": "Sexual relations between closely related individuals.",
    "bestiality": "Sexual acts involving humans and animals.",
    "fetish_content": "Material focused on specific sexual interests or fetishes.",
    "triggering_content": "Content likely to provoke traumatic or intensely negative emotional responses.",
    "incorrect_diagnosis": "Provision of a medical or mental health diagnosis without appropriate expertise.",
    "gaslighting": "Manipulation causing someone to question their own perceptions or sanity.",
    "stigmatizing_language": "Language reinforcing negative stereotypes about mental or physical health.",
    "sexuality": "Themes or content relating to sexual orientation, behavior, or preferences.",
    "mental_health": "Topics involving mental health, well-being, or psychological conditions.",
    "sensitivity": "Content requiring extra care due to the potential for emotional or psychological impact.",

    # ---- Fairness, Justice, Social Harms ----
    "fairness": "Discussion, critique, or promotion of equity and justice across domains.",
    "equality": "Content concerning equal treatment and opportunity for all.",
    "justice": "Content relating to legal, moral, or social justice.",
    "algorithmic_manipulation": "Deliberate bias or unfairness introduced through algorithmic systems.",
    "proxy_discrimination": "Use of correlated non-protected characteristics to indirectly discriminate.",
    "unfair_targeting": "Discriminatory selection or exclusion not justified by policy or law.",
    "redlining": "Denying services or opportunities based on group or neighborhood.",

    # ---- Self-Harm, Suicide, and Vulnerable Groups ----
    "self_harm_methods": "Instructions or graphic descriptions of self-injury or suicide techniques.",
    "suicide_pact": "Promotion or discussion of agreements to die by suicide together.",
    "encouraging_self_harm": "Statements or messages that glorify, promote, or encourage self-injury or suicide.",
    "minor_protection": "Efforts to safeguard minors from harmful, exploitative, or age-inappropriate content.",

    # ---- Spam, Deceptive Content, and Unsolicited Material ----
    "unsolicited_content": "Messages or material sent without the recipient’s request or consent.",
    "phishing": "Attempts to trick users into revealing personal or financial information.",
    "spam_content": "Repetitive, irrelevant, or unwanted digital messages.",
    "link_farm": "Websites or pages with excessive links for manipulative or deceptive purposes.",
    "clickbait": "Sensational headlines or images designed to attract clicks with misleading content.",

    # ---- NEW & EMERGING RISKS ----
    "impersonation": "Pretending to be another person or organization to deceive or defraud.",
    "conspiracy": "Promotion or endorsement of unsubstantiated secret plots or coverups.",
    "extremism": "Promotion, support, or justification of radical ideologies or violence.",
    "gambling": "Promotion, facilitation, or glamorization of betting or games of chance.",
    "financial_advice": "Recommendations regarding financial decisions, often without required credentials or oversight.",
    "scams": "Schemes or deceptive tactics to unlawfully acquire money or sensitive information.",
}


# Define vocabulary globally
vocabulary = ["cathedral", "synagogue", "hospital", "doctor", "clinic", "election", "campaign", "vote", "vaccine",
              "medicine", "pollution", "climate", "recycle", "sustainability", "yoga", "meditation", "bitcoin",
              "finance", "court", "judge", "racism", "equality", "gay", "pride", "data", "security", "opium",
              "methamphetamine", "hash", "kava", "psychedelic"]


def detect_entities(prompt):
    doc = nlp(prompt)
    prompt_lower = prompt.lower()
    entities_set = set()
    entities = []

    # DeBERTa NER
    inputs = ner_tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)
    inputs = {k: v.to(ner_model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = ner_model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=2)
    tokens = ner_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    for token_idx, pred in enumerate(predictions[0]):
        if pred != 0:
            entity = tokens[token_idx].replace("##", "")
            category = next((ctx for ctx, kws in context_keywords.items() if any(kw in entity.lower() for kw in kws)), "general")
            if entity and (entity.lower(), category) not in entities_set:
                entities.append({"entity": entity, "category": category})
                entities_set.add((entity.lower(), category))

    # SpaCy NER
    for ent in doc.ents:
        if ent.label_ in ["PERSON", "GPE", "ORG", "LOC"]:
            if (ent.text.lower(), "general") not in entities_set:
                entities.append({"entity": ent.text, "category": "general"})
                entities_set.add((ent.text.lower(), "general"))

    # Keyword Matching
    for context, keywords in context_keywords.items():
        for keyword in keywords:
            if keyword in prompt_lower and (keyword.lower(), context) not in entities_set:
                entities.append({"entity": keyword, "category": context})
                entities_set.add((keyword.lower(), context))

    return entities

def detect_context(prompt, entities):
    context_labels = set()

    # Entity-based context
    for entity in entities:
        if entity["category"] in keyword_to_category:
            context_labels.update(keyword_to_category[entity["category"]])

    # Semantic similarity
    prompt_emb = model.encode([prompt])[0]
    for context_label, desc in category_descriptions.items():
        desc_emb = model.encode([desc])[0]
        sim = np.dot(prompt_emb, desc_emb) / (np.linalg.norm(prompt_emb) * np.linalg.norm(desc_emb))
        if sim > 0.7:
            context_labels.add(context_label)

    # Toxic-BERT label threshold
    intent_scores = toxicity_classifier(prompt, return_all_scores=True)[0]
    for score in intent_scores:
        if score['label'] in category_descriptions and score['score'] > 0.5:
            context_labels.add(score['label'])

    if not context_labels and entities:
        context_labels.add("general_context")
    return list(context_labels)

def detect_style(prompt):
    styles = [
        "imperative", "interrogative", "declarative", "exclamatory", "narrative", "descriptive", "dialogic",
        "poetic", "satirical", "humorous", "didactic", "reflective", "contemplative", "cautionary", "colloquial",
        "informative", "analytical", "persuasive", "critical", "explanatory", "instructional", "hypothetical",
        "provocative", "sarcastic", "assertive", "apathetic", "emphatic", "emotional", "optimistic", "pessimistic", "neutral"
    ]
    style_scores = toxicity_classifier(prompt, top_k=None)
    style_map = {"imperative": 0.6, "question": 0.7, "narrative": 0.5, "provocative": 0.4}
    predicted_style = max(styles, key=lambda s: style_map.get(s, 0.0) if any(s in score['label'].lower() for score in style_scores) else 0.0)

    tokens = word_tokenize(prompt)
    tagged = pos_tag(tokens)
    if not predicted_style or predicted_style == "narrative":
        if any(pos.startswith('VB') for _, pos in tagged):
            predicted_style = "imperative"
        elif any(pos.startswith('WR') for _, pos in tagged):
            predicted_style = "question"
        else:
            predicted_style = "narrative"
    return predicted_style

def process_prompt(prompt):
    """Process prompt to detect entities, intents, style, and additional labels."""
    entities = detect_entities(prompt)
    entity_all = list({entity['entity'] for entity in entities})  # Unique entity strings
    intents = detect_intents(prompt, entities)
    style = detect_style(prompt)

    # Confidence scores
    confidence_scores = {entity["category"]: 0.8 for entity in entities if entity["category"] != "general"}
    if not confidence_scores and entities:
        confidence_scores = {"general": 0.6}

    # Additional labels
    toxicity_result = toxicity_classifier(prompt)[0]
    additional_labels = {
        "city_presence": {"value": any(ent.label_ == "GPE" for ent in nlp(prompt).ents), "confidence": 0.9},
        "religion_presence": {"value": any(ent["category"] == "religion" for ent in entities), "confidence": 0.9},
        "toxicity_level": {"value": toxicity_result["label"], "confidence": max(0.5, toxicity_result["score"])},
        "sentiment_score": {"value": TextBlob(prompt).sentiment.polarity, "confidence": 0.7},
        "complexity": {"value": len(word_tokenize(prompt)) / 10, "confidence": 0.6}
    }

    # Attack types by style
    attack_types_by_style = {style: set(intents)}

    return {
        "prompt": prompt,
        "entities": entities,
        "entity_all": ", ".join(entity_all),  # <<--- HERE!
        "context": intents,  # renamed from "intents"
        "style_of_prompt": style,
        "confidence_scores": confidence_scores,
        "additional_labels": additional_labels,
        "attack_types_by_style": attack_types_by_style
    }



try:
    df = pd.read_csv("prompts.csv", usecols=["prompt"]).head(10).dropna()
    if len(df) < 10:
        raise ValueError("CSV must contain at least 10 prompts.")
except FileNotFoundError:
    raise FileNotFoundError("prompts.csv not found. Please create a CSV file with a 'prompt' column containing at least 10 prompts.")
except ValueError as e:
    raise ValueError(f"Error: {e}")

results = []
for prompt in df["prompt"]:
    result = process_prompt(prompt)
    results.append(result)

output_df = pd.DataFrame(results)
output_df.to_csv("context_labeled_outputt21.csv", index=False)
print("Results saved to context_labeled_outputt21.csv")
