{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37f2a4fdaac445d8af3b066f1f7bf8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12e781d1c7124aab8390a3195737beeb",
              "IPY_MODEL_2f6a063244d4422fb5371c8a6ad11bc2",
              "IPY_MODEL_9fc9f8df26aa4da48fb7518c09cf0bfd"
            ],
            "layout": "IPY_MODEL_4a3c861668614593a458407a44862da3"
          }
        },
        "12e781d1c7124aab8390a3195737beeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1364602816a4b6f9867f88593acda60",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9722248061b24a0f811c08cf8f23df9a",
            "value": "pytorch_model.bin:‚Äá100%"
          }
        },
        "2f6a063244d4422fb5371c8a6ad11bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b6d37a7a6943b59bd502793224d5c7",
            "max": 419080819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3c3e61bb549445fb21ab72cc100364b",
            "value": 419080819
          }
        },
        "9fc9f8df26aa4da48fb7518c09cf0bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45081cf8c7894732ab00d4474a1b59b4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5fa451ffa7ce4d87b725363cae909a00",
            "value": "‚Äá419M/419M‚Äá[00:05&lt;00:00,‚Äá52.8MB/s]"
          }
        },
        "4a3c861668614593a458407a44862da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1364602816a4b6f9867f88593acda60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9722248061b24a0f811c08cf8f23df9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b6d37a7a6943b59bd502793224d5c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c3e61bb549445fb21ab72cc100364b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45081cf8c7894732ab00d4474a1b59b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa451ffa7ce4d87b725363cae909a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSsYdMmFPP-G",
        "outputId": "34cd3025-19d7-4ca2-802f-9e15323b3861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Command '['/content/taenv/bin/python3', '-m', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n",
            "/content/taenv/bin/python: No module named pip\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n",
            "/content/taenv/bin/python: No module named spacy\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ModuleNotFoundError: No module named 'nltk'\n",
            "/bin/bash: line 1: /content/taenv/bin/pip: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Create venv\n",
        "!python -m venv /content/taenv\n",
        "!/content/taenv/bin/python -m pip -q install -U pip setuptools wheel\n",
        "\n",
        "# Core numeric stack (stable, ABI-safe)\n",
        "!/content/taenv/bin/pip -q install numpy==1.26.4 pandas==2.2.2 scipy==1.11.4 scikit-learn==1.4.2\n",
        "\n",
        "# Torch (CPU is fine for these augmenters)\n",
        "!/content/taenv/bin/pip -q install torch==2.3.1\n",
        "\n",
        "# spaCy + small English model\n",
        "!/content/taenv/bin/pip -q install spacy==3.8.7\n",
        "!/content/taenv/bin/python -m spacy download en_core_web_sm\n",
        "\n",
        "# HF stack (pin datasets 2.x to avoid TextAttack conflicts)\n",
        "!/content/taenv/bin/pip -q install transformers==4.44.2 tokenizers==0.19.1 accelerate==0.30.1 \\\n",
        "                         sentencepiece==0.2.0 sacremoses==0.1.1 datasets==2.18.0\n",
        "\n",
        "# TextAttack and helpers (NO Flair needed)\n",
        "!/content/taenv/bin/pip -q install textattack==0.3.8 tqdm==4.66.4 xlsxwriter==3.2.0\n",
        "\n",
        "# NLTK (for WordNet); fetch corpora\n",
        "!/content/taenv/bin/pip -q install nltk==3.9.1\n",
        "!/content/taenv/bin/python -c \"import nltk; nltk.download('wordnet'); nltk.download('omw-1.4'); print('NLTK ready')\"\n",
        "\n",
        "# (Optional) for --sim_gating\n",
        "!/content/taenv/bin/pip -q install sentence-transformers==3.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/augment6.py\n",
        "# <-- paste the 6-augmenter script I gave (no BackTranslation, no CLARE, skips `context`) -->\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izmNSmjhPTQX",
        "outputId": "a1cf8e9c-5aff-4346-9be2-e086ba60edac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/augment6.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/taenv/bin/python /content/augment6.py \\\n",
        "  --input /content/advanced_adversarial_prompts.csv \\\n",
        "  --run_semantics --run_lexical --run_entity \\\n",
        "  --n_semantic 2 --n_lexical 2 --n_entity 2 \\\n",
        "  --write_wide\n"
      ],
      "metadata": {
        "id": "0d6sMXw3QH-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Adversarial Prompt Augmentation (6 augmenters, no BT/CLARE)\n",
        "# - Uses: EasyDataAugmenter, WordNetAugmenter, EmbeddingAugmenter,\n",
        "#         CharSwapAugmenter, CheckListAugmenter, Entity-swap\n",
        "# - Skips 'context' column\n",
        "# - Auto-enables groups if none passed\n",
        "# - Optional semantic similarity gating (--sim_gating)\n",
        "# ============================================\n",
        "import os, re, json, random, uuid, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- spaCy for entity-centric swaps (self-healing load)\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    try:\n",
        "        from spacy.cli import download\n",
        "        download(\"en_core_web_sm\")\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load/download spaCy 'en_core_web_sm': {e}\")\n",
        "\n",
        "# ---- Optional semantic similarity gating (off by default)\n",
        "_ST_MODEL = None\n",
        "def _lazy_load_st():\n",
        "    global _ST_MODEL\n",
        "    if _ST_MODEL is None:\n",
        "        try:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            _ST_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è sentence-transformers not available for sim gating: {e}\")\n",
        "            _ST_MODEL = False\n",
        "    return _ST_MODEL\n",
        "\n",
        "# ---- TextAttack augmenters (0.3.8)\n",
        "from textattack.augmentation import (\n",
        "    EasyDataAugmenter,\n",
        "    WordNetAugmenter,\n",
        "    EmbeddingAugmenter,\n",
        "    CharSwapAugmenter,\n",
        "    CheckListAugmenter,\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# Defaults\n",
        "# ============================\n",
        "DEFAULT_INPUT = \"advanced_adversarial_prompts.csv\"\n",
        "DEFAULT_OUTPUT_LONG = \"prompts_adversarial_augmented_long.csv\"\n",
        "DEFAULT_OUTPUT_WIDE = \"prompts_adversarial_augmented_wide.xlsx\"\n",
        "\n",
        "# Only these columns get augmented (explicitly skip 'context')\n",
        "SOURCE_COLUMNS = [\n",
        "    \"original_prompt\", \"amplified\", \"paradoxical\",\n",
        "    \"ambiguous\", \"entity_mixup\", \"cot_escalation\"\n",
        "]\n",
        "SKIP_COLUMNS = {\"context\"}\n",
        "\n",
        "# CoT splitting\n",
        "COT_SPLIT_REGEX = r\"\\s*\\|\\|\\|\\s*|\\s*\\|\\|\\s*|###|\\n{2,}\"\n",
        "\n",
        "# ============================\n",
        "# Entity pools (neutral)\n",
        "# ============================\n",
        "ENTITY_POOLS = {\n",
        "    \"PERSON_FIRST\": [\n",
        "        \"Alex\",\"Jordan\",\"Taylor\",\"Casey\",\"Riley\",\"Sam\",\"Avery\",\"Drew\",\"Cameron\",\"Morgan\",\n",
        "        \"Jamie\",\"Quinn\",\"Parker\",\"Sydney\",\"Rowan\"\n",
        "    ],\n",
        "    \"PERSON_LAST\": [\n",
        "        \"Smith\",\"Patel\",\"Garcia\",\"Lee\",\"Khan\",\"Johnson\",\"Kim\",\"Singh\",\"Nguyen\",\"Hernandez\",\n",
        "        \"Brown\",\"Davis\",\"Martinez\",\"Wilson\",\"Anderson\"\n",
        "    ],\n",
        "    \"ORG\": [\n",
        "        \"Northbridge Analytics\",\"Orion Labs\",\"Cedar Systems\",\"Atlas Research\",\n",
        "        \"Summit Media\",\"BlueRiver Health\",\"Vertex Finance\",\"Nimbus AI\"\n",
        "    ],\n",
        "    \"GPE\": [\"Canada\",\"India\",\"Brazil\",\"Japan\",\"Germany\",\"Kenya\",\"Spain\",\"UAE\",\"Australia\",\"Mexico\"],\n",
        "    \"NORP\": [\"engineers\",\"artists\",\"journalists\",\"researchers\",\"students\",\"teachers\",\n",
        "             \"nurses\",\"developers\",\"lawyers\",\"scientists\"],\n",
        "}\n",
        "def _rand_person():\n",
        "    return f\"{random.choice(ENTITY_POOLS['PERSON_FIRST'])} {random.choice(ENTITY_POOLS['PERSON_LAST'])}\"\n",
        "\n",
        "# ============================\n",
        "# Augmenter factories (6 total)\n",
        "# ============================\n",
        "def build_semantics_augmenters():\n",
        "    # (1) EDA (light paraphrasing: insertion/deletion/swap/synonym)\n",
        "    # (2) WordNet synonyms\n",
        "    return [\n",
        "        (\"semantics.eda\", EasyDataAugmenter(pct_words_to_swap=0.1, transformations_per_example=8)),\n",
        "        (\"semantics.wordnet\", WordNetAugmenter(pct_words_to_swap=0.2)),\n",
        "    ]\n",
        "\n",
        "def build_lexical_char_augmenters():\n",
        "    # (3) Embedding swaps\n",
        "    # (4) Character swaps (DeepWordBug-like)\n",
        "    # (5) CheckList invariances (case/punct/etc.)\n",
        "    return [\n",
        "        (\"lexical.embedding\", EmbeddingAugmenter(pct_words_to_swap=0.2)),\n",
        "        (\"char.charswap\", CharSwapAugmenter()),\n",
        "        (\"lexical.checklist\", CheckListAugmenter()),\n",
        "    ]\n",
        "\n",
        "# (6) Entity-centric swap helper\n",
        "def entity_swap_variants(text: str, n: int = 2):\n",
        "    doc = nlp(text)\n",
        "    variants = []\n",
        "    for _ in range(n):\n",
        "        rep = {}\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\":\n",
        "                rep[ent.text] = _rand_person()\n",
        "            elif ent.label_ == \"ORG\":\n",
        "                rep[ent.text] = random.choice(ENTITY_POOLS[\"ORG\"])\n",
        "            elif ent.label_ == \"GPE\":\n",
        "                rep[ent.text] = random.choice(ENTITY_POOLS[\"GPE\"])\n",
        "            elif ent.label_ == \"NORP\":\n",
        "                rep[ent.text] = random.choice(ENTITY_POOLS[\"NORP\"])\n",
        "        new_text = text\n",
        "        if rep:\n",
        "            # replace longest-first to avoid partial overlaps\n",
        "            for k in sorted(rep.keys(), key=len, reverse=True):\n",
        "                new_text = re.sub(rf\"\\b{re.escape(k)}\\b\", rep[k], new_text)\n",
        "            variants.append(new_text)\n",
        "        else:\n",
        "            variants.append(f\"{_rand_person()} mentioned that {text}\")\n",
        "    # dedup\n",
        "    seen, uniq = set(), []\n",
        "    for v in variants:\n",
        "        v2 = v.strip()\n",
        "        if v2 and v2 not in seen:\n",
        "            seen.add(v2); uniq.append(v2)\n",
        "    return uniq[:n]\n",
        "\n",
        "# ============================\n",
        "# Helpers\n",
        "# ============================\n",
        "def split_cot_field(value):\n",
        "    if pd.isna(value):\n",
        "        return []\n",
        "    s = str(value).strip()\n",
        "    if s.startswith(\"[\"):\n",
        "        try:\n",
        "            arr = json.loads(s)\n",
        "            return [str(x).strip() for x in arr if str(x).strip()]\n",
        "        except Exception:\n",
        "            pass\n",
        "    parts = re.split(COT_SPLIT_REGEX, s)\n",
        "    parts = [p.strip() for p in parts if p and p.strip()]\n",
        "    if len(parts) == 1 and \"\\n\" in s:\n",
        "        lines = [ln.strip() for ln in s.split(\"\\n\") if ln.strip()]\n",
        "        if 2 <= len(lines) <= 5:\n",
        "            parts = lines\n",
        "    return parts\n",
        "\n",
        "def passes_basic_gates(original: str, augmented: str, min_tokens: int) -> bool:\n",
        "    if not augmented:\n",
        "        return False\n",
        "    o = original.strip()\n",
        "    a = augmented.strip()\n",
        "    if not a or a.lower() == o.lower():\n",
        "        return False\n",
        "    if len(a.split()) < min_tokens:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def sim_gate_ok(original: str, augmented: str, sim_thresh: float, cache: dict) -> bool:\n",
        "    if sim_thresh is None:\n",
        "        return True\n",
        "    model = _lazy_load_st()\n",
        "    if not model:\n",
        "        return True\n",
        "    if original not in cache:\n",
        "        cache[original] = model.encode([original], normalize_embeddings=True)[0]\n",
        "    if augmented not in cache:\n",
        "        cache[augmented] = model.encode([augmented], normalize_embeddings=True)[0]\n",
        "    v1, v2 = cache[original], cache[augmented]\n",
        "    return float(np.dot(v1, v2)) >= sim_thresh\n",
        "\n",
        "def safe_augment(augmenter, text: str, k: int):\n",
        "    try:\n",
        "        out = augmenter.augment(text)\n",
        "        if not out:\n",
        "            return []\n",
        "        uniq = list(dict.fromkeys([o.strip() for o in out if isinstance(o, str) and o.strip()]))\n",
        "        random.shuffle(uniq)\n",
        "        return uniq[:k]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# ============================\n",
        "# Optional wide writer\n",
        "# ============================\n",
        "def write_wide_xlsx(long_df: pd.DataFrame, out_path: str, source_cols: list):\n",
        "    key_cols = [\"source_row_idx\", \"source_column\", \"augmented_text\"]\n",
        "    subset = long_df[key_cols].copy()\n",
        "    agg = subset.groupby([\"source_row_idx\", \"source_column\"])[\"augmented_text\"] \\\n",
        "                .apply(lambda xs: \"\\n\".join(list(dict.fromkeys(xs)))) \\\n",
        "                .reset_index()\n",
        "    rows = []\n",
        "    max_row = int(long_df[\"source_row_idx\"].max()) if not long_df.empty else -1\n",
        "    for ridx in range(max_row + 1):\n",
        "        row_dict = {\"row_index\": ridx}\n",
        "        for col in source_cols:\n",
        "            val = agg[(agg[\"source_row_idx\"] == ridx) & (agg[\"source_column\"] == col)]\n",
        "            row_dict[col] = \"\" if val.empty else val[\"augmented_text\"].values[0]\n",
        "        rows.append(row_dict)\n",
        "    wide_df = pd.DataFrame(rows)\n",
        "    with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
        "        wide_df.to_excel(writer, index=False, sheet_name=\"augmented\")\n",
        "    return wide_df\n",
        "\n",
        "# ============================\n",
        "# Main\n",
        "# ============================\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Adversarial augmentation (6 augmenters, no BT/CLARE).\")\n",
        "    ap.add_argument(\"--input\", default=DEFAULT_INPUT, help=\"Input CSV file\")\n",
        "    ap.add_argument(\"--output_long\", default=DEFAULT_OUTPUT_LONG, help=\"Output long CSV filename\")\n",
        "    ap.add_argument(\"--output_wide\", default=DEFAULT_OUTPUT_WIDE, help=\"Output wide XLSX filename\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "\n",
        "    # Which groups to run\n",
        "    ap.add_argument(\"--run_semantics\", action=\"store_true\",\n",
        "                    help=\"Enable semantics-preserving group (EDA + WordNet)\")\n",
        "    ap.add_argument(\"--run_lexical\", action=\"store_true\",\n",
        "                    help=\"Enable lexical/char group (Embedding/CharSwap/CheckList)\")\n",
        "    ap.add_argument(\"--run_entity\", action=\"store_true\",\n",
        "                    help=\"Enable entity-centric group (NER swaps)\")\n",
        "\n",
        "    # Per-augmenter caps\n",
        "    ap.add_argument(\"--n_semantic\", type=int, default=2, help=\"Samples per semantic augmenter\")\n",
        "    ap.add_argument(\"--n_lexical\", type=int, default=2, help=\"Samples per lexical/char augmenter\")\n",
        "    ap.add_argument(\"--n_entity\", type=int, default=2, help=\"Entity swap variants per text\")\n",
        "\n",
        "    # Gates\n",
        "    ap.add_argument(\"--min_tokens\", type=int, default=5, help=\"Minimum tokens in augmented text\")\n",
        "\n",
        "    # Optional similarity gating\n",
        "    ap.add_argument(\"--sim_gating\", action=\"store_true\", help=\"Enable semantic similarity gating\")\n",
        "    ap.add_argument(\"--sim_thresh\", type=float, default=0.85, help=\"Cosine threshold if --sim_gating\")\n",
        "\n",
        "    # Optional wide\n",
        "    ap.add_argument(\"--write_wide\", action=\"store_true\", help=\"Write wide XLSX summary\")\n",
        "\n",
        "    # For notebook: parse empty list; for CLI: change to ap.parse_args()\n",
        "    args = ap.parse_args([])\n",
        "\n",
        "    # Auto-enable all groups if none were specified\n",
        "    if not (args.run_semantics or args.run_lexical or args.run_entity):\n",
        "        args.run_semantics = args.run_lexical = args.run_entity = True\n",
        "\n",
        "    # Seed\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    # Read CSV with encoding fallback\n",
        "    try:\n",
        "        df = pd.read_csv(args.input)\n",
        "    except UnicodeDecodeError:\n",
        "        df = pd.read_csv(args.input, encoding=\"latin1\")\n",
        "\n",
        "    present = [c for c in SOURCE_COLUMNS if c in df.columns]\n",
        "    cols = [c for c in present if c not in SKIP_COLUMNS]\n",
        "    if not cols:\n",
        "        raise ValueError(f\"No expected columns found in {args.input}. Got: {list(df.columns)}\")\n",
        "\n",
        "    print(\"üîé Found columns:\", list(df.columns))\n",
        "    print(\"‚úÖ Will augment:\", cols)\n",
        "    print(\"‚ûï Active groups:\",\n",
        "          \"semantics\" if args.run_semantics else \"-\",\n",
        "          \"lexical\"   if args.run_lexical   else \"-\",\n",
        "          \"entity\"    if args.run_entity    else \"-\")\n",
        "\n",
        "    sem_aug = build_semantics_augmenters() if args.run_semantics else []\n",
        "    lex_aug = build_lexical_char_augmenters() if args.run_lexical else []\n",
        "    run_entity = args.run_entity\n",
        "\n",
        "    sim_cache = {}\n",
        "    if args.sim_gating:\n",
        "        _lazy_load_st()\n",
        "\n",
        "    records = []\n",
        "    total_cells = 0\n",
        "\n",
        "    for ridx, row in tqdm(df.iterrows(), total=len(df), desc=\"Augmenting\"):\n",
        "        for col in cols:\n",
        "            raw = row[col]\n",
        "            if pd.isna(raw):\n",
        "                continue\n",
        "            texts = split_cot_field(raw) if col == \"cot_escalation\" else [str(raw)]\n",
        "            for sub_idx, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if not text:\n",
        "                    continue\n",
        "                total_cells += 1\n",
        "                base_id = str(uuid.uuid4())\n",
        "\n",
        "                # (1) Semantics (EDA + WordNet)\n",
        "                for name, aug in sem_aug:\n",
        "                    outs = safe_augment(aug, text, k=args.n_semantic)\n",
        "                    for out in outs:\n",
        "                        if not passes_basic_gates(text, out, args.min_tokens):\n",
        "                            continue\n",
        "                        if not sim_gate_ok(text, out, args.sim_thresh if args.sim_gating else None, sim_cache):\n",
        "                            continue\n",
        "                        records.append({\n",
        "                            \"source_row_idx\": ridx,\n",
        "                            \"source_column\": col,\n",
        "                            \"cot_index\": sub_idx if col == \"cot_escalation\" else None,\n",
        "                            \"group\": \"semantics_preserving\",\n",
        "                            \"augmenter\": name,\n",
        "                            \"original_text\": text,\n",
        "                            \"augmented_text\": out,\n",
        "                            \"seed\": args.seed,\n",
        "                            \"uuid\": base_id\n",
        "                        })\n",
        "\n",
        "                # (2) Lexical / char\n",
        "                for name, aug in lex_aug:\n",
        "                    outs = safe_augment(aug, text, k=args.n_lexical)\n",
        "                    for out in outs:\n",
        "                        if not passes_basic_gates(text, out, args.min_tokens):\n",
        "                            continue\n",
        "                        if not sim_gate_ok(text, out, args.sim_thresh if args.sim_gating else None, sim_cache):\n",
        "                            continue\n",
        "                        records.append({\n",
        "                            \"source_row_idx\": ridx,\n",
        "                            \"source_column\": col,\n",
        "                            \"cot_index\": sub_idx if col == \"cot_escalation\" else None,\n",
        "                            \"group\": \"lexical_char\",\n",
        "                            \"augmenter\": name,\n",
        "                            \"original_text\": text,\n",
        "                            \"augmented_text\": out,\n",
        "                            \"seed\": args.seed,\n",
        "                            \"uuid\": base_id\n",
        "                        })\n",
        "\n",
        "                # (3) Entity-centric\n",
        "                if run_entity:\n",
        "                    outs = entity_swap_variants(text, n=args.n_entity)\n",
        "                    for out in outs:\n",
        "                        if not passes_basic_gates(text, out, args.min_tokens):\n",
        "                            continue\n",
        "                        if not sim_gate_ok(text, out, args.sim_thresh if args.sim_gating else None, sim_cache):\n",
        "                            continue\n",
        "                        records.append({\n",
        "                            \"source_row_idx\": ridx,\n",
        "                            \"source_column\": col,\n",
        "                            \"cot_index\": sub_idx if col == \"cot_escalation\" else None,\n",
        "                            \"group\": \"entity_centric\",\n",
        "                            \"augmenter\": \"ner.swap\",\n",
        "                            \"original_text\": text,\n",
        "                            \"augmented_text\": out,\n",
        "                            \"seed\": args.seed,\n",
        "                            \"uuid\": base_id\n",
        "                        })\n",
        "\n",
        "    out_df = pd.DataFrame.from_records(records)\n",
        "    out_df.to_csv(args.output_long, index=False)\n",
        "    print(f\"‚úÖ Saved {len(out_df):,} augmented rows to: {args.output_long} (from {total_cells} source cells)\")\n",
        "\n",
        "    if args.write_wide and len(out_df):\n",
        "        write_wide_xlsx(out_df, args.output_wide, cols)\n",
        "        print(f\"üßæ Also wrote wide Excel to: {args.output_wide}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399,
          "referenced_widgets": [
            "37f2a4fdaac445d8af3b066f1f7bf8c3",
            "12e781d1c7124aab8390a3195737beeb",
            "2f6a063244d4422fb5371c8a6ad11bc2",
            "9fc9f8df26aa4da48fb7518c09cf0bfd",
            "4a3c861668614593a458407a44862da3",
            "f1364602816a4b6f9867f88593acda60",
            "9722248061b24a0f811c08cf8f23df9a",
            "80b6d37a7a6943b59bd502793224d5c7",
            "f3c3e61bb549445fb21ab72cc100364b",
            "45081cf8c7894732ab00d4474a1b59b4",
            "5fa451ffa7ce4d87b725363cae909a00"
          ]
        },
        "id": "N7maQ1j5QK-8",
        "outputId": "dd9059c9-a752-45fe-b050-036b2f8524e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Found columns: ['original_prompt', 'context', 'amplified', 'paradoxical', 'ambiguous', 'entity_mixup', 'cot_escalation']\n",
            "‚úÖ Will augment: ['original_prompt', 'amplified', 'paradoxical', 'ambiguous', 'entity_mixup', 'cot_escalation']\n",
            "‚ûï Active groups: semantics lexical entity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 481M/481M [00:10<00:00, 47.4MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmpny8l8o2f.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n",
            "Augmenting:   0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/419M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37f2a4fdaac445d8af3b066f1f7bf8c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-17 13:49:48,492 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Augmenting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [1:15:34<00:00, 90.70s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 2,433 augmented rows to: prompts_adversarial_augmented_long.csv (from 321 source cells)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textattack.augmentation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-SgreazQZNN",
        "outputId": "6467b387-4747-4750-c750-74ab26c4310d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement textattack.augmentation (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for textattack.augmentation\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xch-PYx-QjMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The six augmenters\n",
        "1) Semantics-preserving (factory build_semantics_augmenters)\n",
        "\n",
        "EasyDataAugmenter (semantics.eda): small swaps/insertions/deletions/synonym replacements; configured with pct_words_to_swap=0.1, transformations_per_example=8.\n",
        "\n",
        "WordNetAugmenter (semantics.wordnet): synonym substitutions based on WordNet; pct_words_to_swap=0.2.\n",
        "\n",
        "2) Lexical / character (factory build_lexical_char_augmenters)\n",
        "\n",
        "EmbeddingAugmenter (lexical.embedding): replaces words with nearest neighbors in an embedding space; pct_words_to_swap=0.2.\n",
        "\n",
        "CharSwapAugmenter (char.charswap): character-level operations (insert/delete/substitute/swap) to simulate typos/adversarial character noise.\n",
        "\n",
        "CheckListAugmenter (lexical.checklist): invariance transforms (e.g., punctuation/casing/contractions) based on CheckList methodology.\n",
        "\n",
        "The exact low-level behavior follows TextAttack‚Äôs implementations; the script treats them as black boxes.\n",
        "\n",
        "3) Entity-centric swap (entity_swap_variants)\n",
        "\n",
        "Parses text with spaCy NER once per source text and builds a replacement dict:\n",
        "\n",
        "PERSON ‚Üí a random neutral first+last pair (_rand_person()),\n",
        "\n",
        "ORG ‚Üí a random org from pool,\n",
        "\n",
        "GPE ‚Üí a random geopolitical entity,\n",
        "\n",
        "NORP ‚Üí a random demonym/collective (e.g., ‚Äúengineers‚Äù).\n",
        "\n",
        "Longest-first replacement: sorts keys by length desc and uses \\b...\\b word-boundary regex. This avoids replacing substrings inside larger names (‚ÄúAnn‚Äù inside ‚ÄúAnnabelle‚Äù).\n",
        "\n",
        "If the doc has no entities, it falls back to prefixing the text with a neutral speaker stub:\n",
        "\n",
        "\"Alex Smith mentioned that <original text>\" (with a randomized first/last).\n",
        "\n",
        "Deduplicates variants; returns up to n unique variants."
      ],
      "metadata": {
        "id": "x1rGZbfPcS93"
      }
    }
  ]
}